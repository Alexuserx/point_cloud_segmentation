{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестовое задание\n",
    "\n",
    "### Задача\n",
    "\n",
    "Проезжающие машины в дождливое время образуют вокруг себя облако брызг, которые препятствуют нормальной езде беспилотного автомобиля.\n",
    "Требуется предложить и реализовать модель, которая будет классифицировать точки на 3 класса:\n",
    "- 0: фон\n",
    "- 1: машина\n",
    "- 2: шум (брызги и выхлопные газы)\n",
    "\n",
    "Для обучения и тестирования модели рекомендуется использовать датасет [semantic_spray_dataset](https://github.innominds.com/aldipiroli/semantic_spray_dataset).  \n",
    "В архиве приведен jupyter-ноутбук с примером загрузки облака с разметкой из этого датасета и их дальнейшей визуализацией.\n",
    "\n",
    "Упрощения, которые можно использовать:\n",
    "- допускается рассматривать точки только в непосредственной близости от автомобиля (например, x в пределах [-20; 20], y в пределах [-20, 20])\n",
    "- допускается удалять точки земли по порогу, RANSAC'ом или иным способом\n",
    "- допускается предварительно использовать алгоритм кластеризации (например, DBSCAN) и дальше определять классифицировать кластеры вместо точек.\n",
    "\n",
    "Требования:\n",
    "- Язык реализации python\n",
    "- Время работы алгоритма: <= 50 мс (допускается до 100 мс)\n",
    "\n",
    "Формат ответа:\n",
    "- В качестве ответа требуется прикрепить ссылку на гитхаб с решением или загрузить код файлом или архивом.  \n",
    "- Также нужно приложить 1-2 скрина с качеством работы модели.  \n",
    "- Если модель обучалась, то будут полезны метрики обученной модели (mAP или иные)\n",
    "\n",
    "Срок выполнения задачи - 1 неделя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from contextlib import contextmanager\n",
    "from typing import List, Union, Tuple\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer():\n",
    "    start = time.time()\n",
    "    try:\n",
    "        yield \n",
    "    finally:\n",
    "        print(time.time() - start)\n",
    "\n",
    "visualization_params = dict(\n",
    "    front=[-0.23497, -0.6329, 0.7376],\n",
    "    lookat=[0.0016, 0.0025, -1.1011],\n",
    "    up=[0.5962, 0.5054, 0.6236],\n",
    "    zoom=0.28\n",
    "    )\n",
    "\n",
    "def show(points, labels, bb_vehicle=False):\n",
    "    \"\"\"Функция для визулизации облака точек (3 класса точек)\n",
    "    с возможностью визуализации бокса для клсса автомобиля\"\"\"\n",
    "    colors = np.zeros((labels.size, 3))\n",
    "    colors[labels == 0, 0] = 1. # background - red\n",
    "    colors[labels == 1, 1] = 1. # foreground (vehicle) - green\n",
    "    colors[labels == 2, 2] = 1. # noise - blue\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points[:, :3])\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    visuals = [pcd]\n",
    "    if bb_vehicle:\n",
    "        cluster = pcd.select_by_index(np.where(labels == 1)[0])\n",
    "        bounding_box = cluster.get_axis_aligned_bounding_box()\n",
    "        bounding_box.color = [0, 0, 0]\n",
    "        visuals.append(bounding_box)\n",
    "\n",
    "    o3d.visualization.draw_geometries(visuals, **visualization_params)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы трансформации (преобразования) облака точек, которые будут использоваться в дальнейшем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudTransform(ABC):\n",
    "    \"\"\"Базовый класс преобразования облака точек\"\"\"\n",
    "    def __init__(self):\n",
    "        pass \n",
    "\n",
    "    def __call__(self, points, labels):\n",
    "        return self.transform(points, labels)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transform(self, points, labels):\n",
    "        pass \n",
    "\n",
    "class RandomRotateZ(PointCloudTransform):\n",
    "    \"\"\"Класс поворота облака точек вокруг оси Oz на случайный угол\"\"\"\n",
    "    def __init__(self):\n",
    "        super(RandomRotateZ, self).__init__()\n",
    "\n",
    "    def transform(self, points, labels):\n",
    "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cos_val = np.cos(rotation_angle)\n",
    "        sin_val = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cos_val, sin_val, 0],\n",
    "                                    [-sin_val, cos_val, 0],\n",
    "                                    [0, 0, 1]])\n",
    "        points[:, :3] = np.dot(points[:, :3], rotation_matrix)\n",
    "        return points, labels \n",
    "\n",
    "class CenterCropXY(PointCloudTransform):\n",
    "    \"\"\"Класс вырезания части облака точек заданного размера (по осям Ox и Oy)\"\"\"\n",
    "    def __init__(self, \n",
    "                 sizes: Union[List[float], Tuple[float], np.ndarray]):\n",
    "        super(CenterCropXY, self).__init__()\n",
    "        self.sizes = sizes\n",
    "\n",
    "    def transform(self, points, labels):\n",
    "        mask = (np.abs(points[:, 0]) <= self.sizes[0]) & (np.abs(points[:, 1]) <= self.sizes[1])\n",
    "        return points[mask, :], labels[mask]\n",
    "\n",
    "class CenterCrop(CenterCropXY):\n",
    "    \"\"\"Класс вырезания симметричной части облака точек заданного размера (по осям Ox и Oy)\"\"\"\n",
    "    def __init__(self, size):\n",
    "        super(CenterCrop, self).__init__([size, size])\n",
    "    \n",
    "class MinMaxScaler(PointCloudTransform):\n",
    "    \"\"\"Класс Min-Max нормализации облака точек\"\"\"\n",
    "    def __init__(self, \n",
    "                 mins: Union[List[float], Tuple[float], np.ndarray], \n",
    "                 maxs: Union[List[float], Tuple[float], np.ndarray]):\n",
    "        self.mins = np.array(mins)\n",
    "        self.maxs = np.array(maxs)\n",
    "    \n",
    "    def transform(self, points, labels):\n",
    "        # points[:, :3] = (points[:, :3] - self.mins) / (self.maxs - self.mins)\n",
    "        n = points.shape[1]\n",
    "        points = (points - self.mins[:n]) / (self.maxs[:n] - self.mins[:n])\n",
    "        return points, labels\n",
    "\n",
    "class Compose(PointCloudTransform):\n",
    "    \"\"\"Класс последовательного применения трансформаций облака точек\"\"\"\n",
    "    def __init__(self, transfroms: List[PointCloudTransform]):\n",
    "        self.transfroms = transfroms\n",
    "\n",
    "    def transform(self, points, labels):\n",
    "        for transfrom in self.transfroms:\n",
    "            points, labels = transfrom(points, labels)\n",
    "        return points, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс, определяющий датасет SemanticSprayDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSprayDataset(Dataset):\n",
    "    \"\"\"Датасет, определяющий облако точек для обучениния молелей\n",
    "    Разбиение на обучающую и валидационную выборки определяется файлами ImageSets/train.txt и ImageSets/test.txt,\n",
    "    представленными в репозитори датасета https://github.innominds.com/aldipiroli/semantic_spray_dataset\n",
    "    Часть определения датасета позаимствована из вышеуказанного репозитория. \n",
    "    Params:\n",
    "        root_path (str) - путь к данным \n",
    "        split (str) - train или test\n",
    "        return_xyz (bool) - флаг, определяющий вернуть все каналы или только координатные \n",
    "        sample_size (int) - размер возвращаемого облака точек; точки случайно сэмплируются \n",
    "            из исходного облака с возвращением или без в зависимости от размера исходного облака.\n",
    "            Если значение параметра None, то будет возвращено всё облако точек\n",
    "        labelweights - веса лейблов\n",
    "        transfroms - преобразование или набор преобразований облака точек\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 root_path: str, \n",
    "                 split: str = \"train\",\n",
    "                 return_xyz: bool = True,\n",
    "                 sample_size: int = 4096,\n",
    "                 labelweights: Union[np.ndarray, List[float]] = None,\n",
    "                 transfroms: Union[Compose, PointCloudTransform] = None):\n",
    "        self.root_path = root_path\n",
    "        self.split = split\n",
    "        self.return_xyz = return_xyz\n",
    "\n",
    "        split_dir = os.path.join(self.root_path, \"ImageSets\", self.split + \".txt\")\n",
    "        scenes_list = [x.strip() for x in open(split_dir).readlines()]\n",
    "\n",
    "        self.sample_size = sample_size\n",
    "        self.transfroms = transfroms\n",
    "        if labelweights:\n",
    "            self.labelweights = 1 - np.array(labelweights)\n",
    "        self.sample_id_list = self.get_data_samples(scenes_list)\n",
    "\n",
    "    def get_data_samples(self, scenes_list):\n",
    "        all_samples = []\n",
    "        for sample_id in scenes_list:\n",
    "            velo_files = sorted(next(os.walk(os.path.join(self.root_path, sample_id, \"velodyne\")), (None, None, []))[2])\n",
    "            for curr_id in velo_files:\n",
    "                scan_id = curr_id.split(\".\")[0]\n",
    "                all_samples.append(os.path.join(self.root_path, sample_id, scan_id))\n",
    "        return all_samples\n",
    "\n",
    "    def load_data(self, scene_path, scan_id):\n",
    "        velo_path = os.path.join(scene_path, \"velodyne\", scan_id + \".bin\")\n",
    "        points = np.fromfile(velo_path, np.float32).reshape(-1, 5)\n",
    "        labels_path = os.path.join(scene_path, \"labels\", scan_id + \".label\")\n",
    "        labels = np.fromfile(labels_path, np.int32).reshape(-1)  \n",
    "        if self.return_xyz:\n",
    "            return points[:, :3], labels \n",
    "        else:\n",
    "            return points, labels \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_id_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = os.path.join(self.sample_id_list[index])\n",
    "        scan_id = data_path.split(\"\\\\\")[-1] \n",
    "        scene_path = data_path[:-6]\n",
    "        points, labels = self.load_data(scene_path, scan_id)\n",
    "        if self.transfroms:\n",
    "            points, labels = self.transfroms(points, labels)\n",
    "\n",
    "        if self.sample_size is None:\n",
    "            return points, labels\n",
    "        \n",
    "        if labels.size >= self.sample_size:\n",
    "            # Сэмлирование с определенными вероятностями для разных классов [не используется в финальном варианте]\n",
    "            # `Если класс малочисленный, то вероятность его семлирования выше`\n",
    "            # sampling_weights = np.array([self.labelweights[i] for i in labels]) \n",
    "            # sampling_weights = sampling_weights / np.sum(sampling_weights)\n",
    "            # points_idxs = np.random.choice(labels.size, self.sample_size, replace=False, p=sampling_weights)\n",
    "            points_idxs = np.random.choice(labels.size, self.sample_size, replace=False)\n",
    "        else:\n",
    "            points_idxs = np.random.choice(labels.size, self.sample_size, replace=True)\n",
    "\n",
    "        points, labels = points[points_idxs], labels[points_idxs]\n",
    "\n",
    "        return points, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первый метод (RANSAC planar segmentation + DBSCAN clustering)\n",
    "\n",
    "`Сразу стоит отметить, что данный метод предполагает достаточно много условностей и не был проанализирован до конца. Предпочтение было отдано другому методу, рассмотренному в дальнейшем.`\n",
    "\n",
    "Предобработка:\n",
    "- из исходного облака точек вырезается область, для которой $|x| < 30$ и $|y| < 30$ (если вырезать область $|x| < 20$ и $|y| < 20$, то в неё почти никогда не попадает транспортное средство)\n",
    "\n",
    "Предположения (они же упрощения):\n",
    "- предполагается, что машина и шумы находятся в узкой полосе  $-10 < y < 5$ (слева болшее расстояние, так как движение правосторенне). Также определять данную область можно с помощью кластеризации DBSCAN, однако данный метод на практике работает дольше 50ms и сильно чувствителен к гиперпараметрам.\n",
    "- предполагается, что машина - достаточно плотный кластер, который можно отделить от шумов с помощью DBSCAN с достаточно малым парметром $\\varepsilon$ (как будет видно далее, предположение не всегда корректно; например, при очень большом количестве плотных шумов)\n",
    "- предполагается, что центральные точки $|x| < 2$ и $|y| < 2$, отвечающиее за транспортное средство с лидаром, можно по-умолчанию относить к классу 0. В далее представленных методах данный случай не будет обрабатываться отдельно.  \n",
    "\n",
    "Далее будут предложены 2 достаточно близких по сути варианта, отличающихся порядком выполнения действий. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вариант 1:\n",
    "1) С использованием алгоритма RANSAC из облака точек отделяются точки поверхности \n",
    "2) Отделяется центральная полоса (дорога), для которой $-10 < y < 5$. Точки вне дороги относятся к классу 0 (фон)\n",
    "3) Точки внутри полосы (дороги) кластеризуются с использованием алгоритма DBSCAN с малым значением параметра  $\\varepsilon$ для отделения машины от шумов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_cloud(points, labels, transfrom=None):\n",
    "    # ================================================\n",
    "    # crop central area 30 x 30 and create point cloud \n",
    "    # ================================================\n",
    "    if transfrom:\n",
    "        points, labels = transfrom(points, labels)\n",
    "    points, labels = transfrom(points, labels)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points[:, :3])\n",
    "\n",
    "    # ================================================\n",
    "    # remove surface area with RANSAC\n",
    "    # ================================================\n",
    "    dist = np.mean(pcd.compute_nearest_neighbor_distance())\n",
    "    distance_threshold = 2 * dist \n",
    "    _, surface_idxs = pcd.segment_plane(distance_threshold=distance_threshold,\n",
    "                                        ransac_n=10,\n",
    "                                        num_iterations=1000)\n",
    "\n",
    "    surface_cloud = pcd.select_by_index(surface_idxs)\n",
    "    surface_cloud.paint_uniform_color([1, 0, 0]) # surface - red \n",
    "    non_surface_cloud = pcd.select_by_index(surface_idxs, invert=True)\n",
    "    non_surface_cloud.paint_uniform_color([0, 0, 0])\n",
    "\n",
    "    # ================================================\n",
    "    # crop narrow central area by -7 < |y| < 5 \n",
    "    # Due to right-hand traffic left crop is greater\n",
    "    # ================================================\n",
    "    non_surface_points = np.asarray(non_surface_cloud.points)\n",
    "\n",
    "    crop_y_left, crop_y_roght = -7, 5\n",
    "    mask = np.where(((non_surface_points[:, 1] > crop_y_left) & \n",
    "                    (non_surface_points[:, 1] < crop_y_roght)))[0]\n",
    "\n",
    "    inner_cloud = non_surface_cloud.select_by_index(mask)\n",
    "    inner_cloud.paint_uniform_color([0, 0, 1]) \n",
    "    outer_cloud = non_surface_cloud.select_by_index(mask, invert=True)\n",
    "    outer_cloud.paint_uniform_color([1, 0, 0]) \n",
    "\n",
    "    # ================================================\n",
    "    # DBSCAN clustering for inner_cloud (narrow road area)\n",
    "    # ================================================\n",
    "    dist = np.mean(inner_cloud.compute_nearest_neighbor_distance())\n",
    "    eps, min_points = 4 * dist, 30\n",
    "    dbscan_labels = np.array(inner_cloud.cluster_dbscan(eps=eps, \n",
    "                                                        min_points=min_points))\n",
    "    dbscan_colors = np.zeros((dbscan_labels.shape[0], 3))\n",
    "    dbscan_colors[dbscan_labels < 0 , 2] = 1 # noise - blue\n",
    "    dbscan_colors[dbscan_labels >= 0, 1] = 1 # foreground (vehicle) - green\n",
    "    inner_cloud.colors = o3d.utility.Vector3dVector(dbscan_colors[:, :3])\n",
    "\n",
    "    # ================================================\n",
    "    # Bounding Boxes for inner_cloud\n",
    "    # ================================================\n",
    "    bounding_boxes = []\n",
    "    for cluster_idx in np.unique(dbscan_labels):\n",
    "        if cluster_idx == -1:\n",
    "            continue\n",
    "        cluster = inner_cloud.select_by_index(np.where(dbscan_labels == cluster_idx)[0])\n",
    "        bounding_box = cluster.get_axis_aligned_bounding_box()\n",
    "        bounding_box.color = [0, 0, 0]\n",
    "        bounding_boxes.append(bounding_box)\n",
    "\n",
    "    # ================================================\n",
    "    # Visualization\n",
    "    # ================================================\n",
    "    visuals = [surface_cloud, inner_cloud, outer_cloud, *bounding_boxes]\n",
    "    return visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_id = \"000050\"\n",
    "points = np.fromfile(f\"{cloud_id}.bin\", np.float32).reshape(-1, 5)\n",
    "labels = np.fromfile(f\"{cloud_id}.label\", np.int32).reshape(-1)\n",
    "\n",
    "transfrom = CenterCrop(30)\n",
    "points, labels = transfrom(points, labels)\n",
    "show(points, labels, bb_vehicle=True)\n",
    "\n",
    "o3d.visualization.draw_geometries(segment_cloud(points, labels, transfrom=CenterCrop(30)), **visualization_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SemanticSprayDataset(\n",
    "    \"./SemanticSprayDataset/\",\n",
    "    \"train\",\n",
    "    return_xyz=True,\n",
    "    sample_size=None,\n",
    "    transfroms=CenterCrop(30)\n",
    ")\n",
    "\n",
    "test_dataset = SemanticSprayDataset(\n",
    "    \"./SemanticSprayDataset/\",\n",
    "    \"test\",\n",
    "    return_xyz=True,\n",
    "    sample_size=None,\n",
    "    transfroms=CenterCrop(30)\n",
    ")\n",
    "\n",
    "show(*train_dataset[2399], bb_vehicle=True)\n",
    "o3d.visualization.draw_geometries(segment_cloud(*train_dataset[2399], transfrom=CenterCrop(30)), **visualization_params)\n",
    "\n",
    "show(*test_dataset[1399], bb_vehicle=True)\n",
    "o3d.visualization.draw_geometries(segment_cloud(*test_dataset[1399], transfrom=CenterCrop(30)), **visualization_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Оригинальное облако|RANSAC + DBSCAN, V1 (предсказание)|\n",
    "|-|-|\n",
    "|Тестовый сэмпл|Тестовый сэмпл (предсказание)|\n",
    "|![alt](images/ransac_v1_orig.png) | ![alt](images/ransac_v1_pred.png)|\n",
    "|Сэмпл из тестового датсета|Сэмпл из тестового датсета (предсказание)|\n",
    "|![alt](images/ransac_v1_orig_test.png) | ![alt](images/ransac_v1_pred_test.png)|\n",
    "|Сэмпл из обучающего датсета|Сэмпл из обучающего датсета (предсказание)|\n",
    "|![alt](images/ransac_v1_orig_train.png) | ![alt](images/ransac_v1_pred_train.png)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.7 ms ± 951 µs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n",
      "34 ms ± 1.4 ms per loop (mean ± std. dev. of 10 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 -r 10 segment_cloud(points, labels, transfrom=CenterCrop(20))\n",
    "%timeit -n 10 -r 10 segment_cloud(points, labels, transfrom=CenterCrop(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среднее время на одно предсказание для модели RANSAC + DBSCAN (Intel i9 12900K):\n",
    "1) для облака точек в области $|x| < 20$ и $|y| < 20$ ~ 28ms\n",
    "2) для облака точек в области $|x| < 30$ и $|y| < 30$ ~ 34ms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вариант 2:\n",
    "1) Отделяется центральная полоса (дорога), для которой $-10 < y < 5$. Точки вне дороги относятся к классу 0 (фон)\n",
    "2) С использованием алгоритма RANSAC из центраьной полосы отделяются точки поверхности \n",
    "3) Точки внутри полосы (дороги) и над поверхностью кластеризуются с использованием алгоритма DBSCAN с малым значением параметра $\\varepsilon$ для отделения машины от шумов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_cloud(points, labels, transfrom=None):\n",
    "    # ================================================\n",
    "    # crop central area 30 x 30 and create point cloud \n",
    "    # ================================================\n",
    "    if transfrom:\n",
    "        points, labels = transfrom(points, labels)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points[:, :3])\n",
    "\n",
    "    # ================================================\n",
    "    # crop narrow central area by -7 < |y| < 5 \n",
    "    # Due to right-hand traffic left crop is greater\n",
    "    # ================================================\n",
    "    crop_y_left, crop_y_roght = -7, 5\n",
    "    mask = np.where(((points[:, 1] > crop_y_left) & \n",
    "                     (points[:, 1] < crop_y_roght)))[0]\n",
    "\n",
    "    inner_cloud = pcd.select_by_index(mask)\n",
    "    inner_cloud.paint_uniform_color([0, 0, 1]) # inner - blue\n",
    "    outer_cloud = pcd.select_by_index(mask, invert=True)\n",
    "    outer_cloud.paint_uniform_color([1, 0, 0]) # outer - blue\n",
    "\n",
    "    # ================================================\n",
    "    # remove surface area for inner_cloud with RANSAC \n",
    "    # ================================================\n",
    "    dist = np.mean(inner_cloud.compute_nearest_neighbor_distance())\n",
    "    distance_threshold = 2 * dist \n",
    "    _, surface_idxs = inner_cloud.segment_plane(distance_threshold=distance_threshold,\n",
    "                                                ransac_n=10,\n",
    "                                                num_iterations=1000)\n",
    "\n",
    "    surface_cloud = inner_cloud.select_by_index(surface_idxs)\n",
    "    surface_cloud.paint_uniform_color([1, 0, 0]) # surface - red \n",
    "    non_surface_cloud = inner_cloud.select_by_index(surface_idxs, invert=True)\n",
    "    non_surface_cloud.paint_uniform_color([0, 0, 0]) # non surface - red \n",
    "\n",
    "    # ================================================\n",
    "    # DBSCAN clustering for non_surface_cloud\n",
    "    # ================================================\n",
    "    dist = np.mean(non_surface_cloud.compute_nearest_neighbor_distance())\n",
    "    eps, min_points = 4 * dist, 30\n",
    "    dbscan_labels = np.array(non_surface_cloud.cluster_dbscan(eps=eps, \n",
    "                                                              min_points=min_points))\n",
    "    dbscan_colors = np.zeros((dbscan_labels.shape[0], 3))\n",
    "    dbscan_colors[dbscan_labels < 0 , 2] = 1 # noise - blue\n",
    "    dbscan_colors[dbscan_labels >= 0, 1] = 1 # foreground (vehicle) - green\n",
    "    non_surface_cloud.colors = o3d.utility.Vector3dVector(dbscan_colors[:, :3])\n",
    "\n",
    "    # ================================================\n",
    "    # Bounding Boxes for non_surface_cloud\n",
    "    # ================================================\n",
    "    bounding_boxes = []\n",
    "    for cluster_idx in np.unique(dbscan_labels):\n",
    "        if cluster_idx == -1:\n",
    "            continue\n",
    "        cluster = non_surface_cloud.select_by_index(np.where(dbscan_labels == cluster_idx)[0])\n",
    "        bounding_box = cluster.get_axis_aligned_bounding_box()\n",
    "        bounding_box.color = [0, 0, 0]\n",
    "        bounding_boxes.append(bounding_box)\n",
    "\n",
    "    # ================================================\n",
    "    # Visualization\n",
    "    # ================================================       \n",
    "    visuals = [outer_cloud, surface_cloud, non_surface_cloud, *bounding_boxes]\n",
    "    return visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_id = \"000050\"\n",
    "points = np.fromfile(f\"{cloud_id}.bin\", np.float32).reshape(-1, 5)\n",
    "labels = np.fromfile(f\"{cloud_id}.label\", np.int32).reshape(-1)\n",
    "\n",
    "transfrom = CenterCrop(30)\n",
    "points, labels = transfrom(points, labels)\n",
    "show(points, labels, bb_vehicle=True)\n",
    "\n",
    "o3d.visualization.draw_geometries(segment_cloud(points, labels, transfrom=CenterCrop(30)), **visualization_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SemanticSprayDataset(\n",
    "    \"./SemanticSprayDataset/\",\n",
    "    \"train\",\n",
    "    return_xyz=True,\n",
    "    sample_size=None,\n",
    "    transfroms=CenterCrop(30)\n",
    ")\n",
    "\n",
    "test_dataset = SemanticSprayDataset(\n",
    "    \"./SemanticSprayDataset/\",\n",
    "    \"test\",\n",
    "    return_xyz=True,\n",
    "    sample_size=None,\n",
    "    transfroms=CenterCrop(30)\n",
    ")\n",
    "\n",
    "show(*train_dataset[2399], bb_vehicle=True)\n",
    "o3d.visualization.draw_geometries(segment_cloud(*train_dataset[2399], transfrom=CenterCrop(30)), **visualization_params)\n",
    "\n",
    "show(*test_dataset[1399], bb_vehicle=True)\n",
    "o3d.visualization.draw_geometries(segment_cloud(*test_dataset[1399], transfrom=CenterCrop(30)), **visualization_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Оригинальное облако|RANSAC + DBSCAN, V2 (предсказание)|\n",
    "|-|-|\n",
    "|Тестовый сэмпл|Тестовый сэмпл (предсказание)|\n",
    "|![alt](images/ransac_v2_orig.png) | ![alt](images/ransac_v2_pred.png)|\n",
    "|Сэмпл из тестового датсета|Сэмпл из тестового датсета (предсказание)|\n",
    "|![alt](images/ransac_v2_orig_test.png) | ![alt](images/ransac_v2_pred_test.png)|\n",
    "|Сэмпл из обучающего датсета|Сэмпл из обучающего датсета (предсказание)|\n",
    "|![alt](images/ransac_v2_orig_train.png) | ![alt](images/ransac_v2_pred_train.png)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.8 ms ± 1.13 ms per loop (mean ± std. dev. of 10 runs, 10 loops each)\n",
      "31.6 ms ± 1.21 ms per loop (mean ± std. dev. of 10 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 -r 10 segment_cloud(points, labels, transfrom=CenterCrop(20))\n",
    "%timeit -n 10 -r 10 segment_cloud(points, labels, transfrom=CenterCrop(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среднее время на одно предсказание для модели RANSAC + DBSCAN (Intel i9 12900K):\n",
    "1) для облака точек в области $|x| < 20$ и $|y| < 20$ ~ 26ms\n",
    "2) для облака точек в области $|x| < 30$ и $|y| < 30$ ~ 32ms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PointNet++ и DGCNN\n",
    "\n",
    "Далее будут рассмотрены 2 deep learning подхода к сегментации облака точек (будут рассмотрены готовые реализации моделей на pytorch с некоторыми доработками). \n",
    "\n",
    "Код модели PointNet++ и цикла её обучения были взяты из репозитория [PointNet++](https://github.com/yanx27/Pointnet_Pointnet2_pytorch)\n",
    "\n",
    "Код модели DGCNN и цикла её обучения были взяты из репозитория [DGCNN](https://github.com/antao97/dgcnn.pytorch)\n",
    "\n",
    "Разбиение датасета на обучающую и тестовую части было взято из репозитория [semantic_spray_dataset](https://github.innominds.com/aldipiroli/semantic_spray_dataset), полный датасет был загружен из источника [Data](https://oparu.uni-ulm.de/xmlui/handle/123456789/48891)\n",
    "\n",
    "Помимо координатных каналов xyz в моделях будут использоваться каналы ring и intensity -> всего 5 каналов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение некоторых характеристик датасета:\n",
    "\n",
    "1) минимальные и максимальные значения координат для нормировки ($|x| < 30$ и $|y| < 30$ вследстивие преобразования `CenterCrop(30)`)\n",
    "2) количество точек для различных лейблов (для получения веса каждго класса с целью дальнейшего использования в функции потерь)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8389/8389 [00:09<00:00, 929.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm \n",
    "    \n",
    "dataset = SemanticSprayDataset(\n",
    "    \"./SemanticSprayDataset/\",\n",
    "    \"train\",\n",
    "    return_xyz=False,\n",
    "    sample_size=4096,\n",
    "    transfroms=CenterCrop(30)\n",
    ")\n",
    "\n",
    "max_all, min_all = [], []\n",
    "labelweights = np.zeros(3)\n",
    "\n",
    "for (points, labels) in tqdm.tqdm(dataset, total=len(dataset)):\n",
    "    tmp, _ = np.histogram(labels, range(4))\n",
    "    labelweights += tmp   \n",
    "    max_all.append(np.amax(points, axis=0)), min_all.append(np.amin(points, axis=0))\n",
    "\n",
    "max_all_np = np.vstack(max_all)\n",
    "min_all_np = np.vstack(min_all)\n",
    "\n",
    "maxs = np.amax(max_all_np, axis=0)\n",
    "mins = np.amin(min_all_np, axis=0)\n",
    "labelweights = labelweights / np.sum(labelweights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Второй метод (модель PointNet++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointnet_pp.train import train\n",
    "\n",
    "\n",
    "class Args:\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_CLASSES = 3\n",
    "    LEARNING_RATE_CLIP = 1e-5\n",
    "    MOMENTUM_ORIGINAL = 0.1\n",
    "    MOMENTUM_DECCAY = 0.5\n",
    "    STEP_SIZE = 5\n",
    "    MOMENTUM_DECCAY_STEP = STEP_SIZE\n",
    "    N_EPOCHS = 17\n",
    "    LR_DECAY = 0.7\n",
    "    LEARNING_RATE = 1e-3\n",
    "    NUM_POINT = 4096\n",
    "    IN_CHANNELS = 5\n",
    "    from_checkpoint = False\n",
    "    maxs = maxs\n",
    "    mins = mins\n",
    "    labelweights = labelweights\n",
    "\n",
    "transfroms = Compose([   \n",
    "    # RandomRotateZ(),\n",
    "    CenterCrop(30),\n",
    "    MinMaxScaler(mins=Args.mins, maxs=Args.maxs) \n",
    "])\n",
    "    \n",
    "train_dataset = SemanticSprayDataset(\n",
    "    \"./SemanticSprayDataset/\",\n",
    "    \"train\",\n",
    "    return_xyz=False,\n",
    "    sample_size=Args.NUM_POINT,\n",
    "    transfroms=transfroms\n",
    ")\n",
    "\n",
    "test_dataset = SemanticSprayDataset(\n",
    "    \"./SemanticSprayDataset/\",\n",
    "    \"test\",\n",
    "    return_xyz=False,\n",
    "    sample_size=Args.NUM_POINT,\n",
    "    transfroms=transfroms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=Args.BATCH_SIZE, shuffle=True, \n",
    "                          drop_last=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Args.BATCH_SIZE, shuffle=False, \n",
    "                         drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [03:58<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0, loss: 0.055339, train acc: 0.990479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [04:23<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0, loss: 0.017476, test acc: 0.996953, test avg acc: 0.983313, test iou: 0.933597\n",
      "------- IoU --------\n",
      "class background     weight: 0.971, IoU: 0.997 \n",
      "class foreground     weight: 0.011, IoU: 0.927 \n",
      "class noise          weight: 0.018, IoU: 0.877 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [03:57<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1, loss: 0.018702, train acc: 0.996686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:42<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1, loss: 0.012708, test acc: 0.997782, test avg acc: 0.986746, test iou: 0.953221\n",
      "------- IoU --------\n",
      "class background     weight: 0.971, IoU: 0.998 \n",
      "class foreground     weight: 0.011, IoU: 0.959 \n",
      "class noise          weight: 0.018, IoU: 0.903 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [04:05<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 2, loss: 0.014894, train acc: 0.997294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:43<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2, loss: 0.014674, test acc: 0.997076, test avg acc: 0.986000, test iou: 0.943534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [03:57<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 3, loss: 0.016911, train acc: 0.997098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:49<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3, loss: 0.013934, test acc: 0.997363, test avg acc: 0.989442, test iou: 0.944133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [04:00<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 4, loss: 0.014084, train acc: 0.997414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:48<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4, loss: 0.018264, test acc: 0.997747, test avg acc: 0.974120, test iou: 0.950462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [03:58<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 5, loss: 0.012530, train acc: 0.997638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:44<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 5, loss: 0.011909, test acc: 0.997690, test avg acc: 0.989925, test iou: 0.949257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [03:58<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 6, loss: 0.014821, train acc: 0.997277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:43<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 6, loss: 0.013658, test acc: 0.997251, test avg acc: 0.987803, test iou: 0.945154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [03:56<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 7, loss: 0.012985, train acc: 0.997547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:46<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 7, loss: 0.012658, test acc: 0.997952, test avg acc: 0.986575, test iou: 0.957017\n",
      "------- IoU --------\n",
      "class background     weight: 0.971, IoU: 0.998 \n",
      "class foreground     weight: 0.011, IoU: 0.962 \n",
      "class noise          weight: 0.018, IoU: 0.911 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [04:07<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 8, loss: 0.012724, train acc: 0.997548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:46<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 8, loss: 0.011663, test acc: 0.998046, test avg acc: 0.988422, test iou: 0.958177\n",
      "------- IoU --------\n",
      "class background     weight: 0.971, IoU: 0.998 \n",
      "class foreground     weight: 0.011, IoU: 0.958 \n",
      "class noise          weight: 0.018, IoU: 0.919 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [04:04<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 9, loss: 0.012230, train acc: 0.997623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:54<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 9, loss: 0.011072, test acc: 0.997946, test avg acc: 0.990846, test iou: 0.956987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [04:00<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 10, loss: 0.011340, train acc: 0.997786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:38<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 10, loss: 0.011822, test acc: 0.998242, test avg acc: 0.987107, test iou: 0.962527\n",
      "------- IoU --------\n",
      "class background     weight: 0.971, IoU: 0.998 \n",
      "class foreground     weight: 0.011, IoU: 0.965 \n",
      "class noise          weight: 0.018, IoU: 0.925 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [03:57<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 11, loss: 0.011360, train acc: 0.997792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:44<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 11, loss: 0.012031, test acc: 0.997965, test avg acc: 0.988163, test iou: 0.957957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [04:05<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 12, loss: 0.012244, train acc: 0.997635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:50<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 12, loss: 0.010950, test acc: 0.997935, test avg acc: 0.990403, test iou: 0.956669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [04:06<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 13, loss: 0.011912, train acc: 0.997643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:41<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 13, loss: 0.014593, test acc: 0.996173, test avg acc: 0.991828, test iou: 0.930539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [03:57<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 14, loss: 0.011044, train acc: 0.997821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:44<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 14, loss: 0.011052, test acc: 0.998175, test avg acc: 0.988861, test iou: 0.961438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [03:59<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 15, loss: 0.010677, train acc: 0.997840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:44<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 15, loss: 0.010420, test acc: 0.998056, test avg acc: 0.991137, test iou: 0.960153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [03:59<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 16, loss: 0.010530, train acc: 0.997899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [03:45<00:00,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 16, loss: 0.010448, test acc: 0.998155, test avg acc: 0.990928, test iou: 0.961291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, test_loader, Args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966851\n"
     ]
    }
   ],
   "source": [
    "from pointnet_pp.model import get_model\n",
    "\n",
    "\n",
    "model = get_model(Args.IN_CHANNELS, Args.NUM_CLASSES).cuda()\n",
    "model.load_state_dict(torch.load(\"./models/pointnet_pp_model_best.pth\")[\"model_state_dict\"])\n",
    "model.eval()\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966851\n"
     ]
    }
   ],
   "source": [
    "from pointnet_pp.model import get_model\n",
    "\n",
    "\n",
    "model = get_model(Args.IN_CHANNELS, Args.NUM_CLASSES).cuda()\n",
    "model.load_state_dict(torch.load(\"./models/pointnet_pp_model_best.pth\")[\"model_state_dict\"])\n",
    "model.eval()\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_one(model, points, labels, args=Args, transfrom=CenterCrop(30)):\n",
    "    \"\"\"Предсказание производится батчами размером по args.NUM_POINT для PointNet++. \n",
    "    Данный размер облака точек использовался во время обучения. \n",
    "    \"\"\"\n",
    "    points = points[:, :args.IN_CHANNELS]\n",
    "    if transfrom:\n",
    "        points, labels = transfrom(points, labels)\n",
    "\n",
    "    idxs = np.random.permutation(np.arange(1, len(labels)))\n",
    "    points, labels = points[idxs, :], labels[idxs]\n",
    "\n",
    "    n_points = points.shape[0]\n",
    "    prop_size = args.NUM_POINT - (n_points % args.NUM_POINT)\n",
    "    points_prop = np.zeros((n_points + prop_size, points.shape[1]))\n",
    "    points_prop[:n_points, :] = points\n",
    "    points_prop[n_points:, :] = points[np.random.choice(labels.size, prop_size, replace=False), :]\n",
    "    \n",
    "    preds_all = []\n",
    "    for i in range(0, n_points, args.NUM_POINT):\n",
    "\n",
    "        points_sample = points_prop[i:i+args.NUM_POINT, :]\n",
    "        points_sample = MinMaxScaler(mins=args.mins, maxs=args.maxs)(points_sample, labels)[0][None, :, :]\n",
    "\n",
    "        points_sample = torch.Tensor(points_sample)\n",
    "        points_sample = points_sample.float().cuda()\n",
    "        points_sample = points_sample.transpose(2, 1)\n",
    "\n",
    "        seg_pred, _ = model(points_sample)\n",
    "        pred_val = seg_pred.contiguous().cpu().data.numpy()\n",
    "\n",
    "        pred_val = np.argmax(pred_val, 2)[0]\n",
    "        preds_all.append(pred_val)\n",
    "\n",
    "    return points, np.hstack(preds_all)[:n_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_id = \"000050\"\n",
    "points = np.fromfile(f\"{cloud_id}.bin\", np.float32).reshape(-1, 5)\n",
    "labels = np.fromfile(f\"{cloud_id}.label\", np.int32).reshape(-1)\n",
    "\n",
    "points_s, pred_labels = predict_one(model, points, labels, transfrom=CenterCrop(30))\n",
    "    \n",
    "transfrom = CenterCrop(30)\n",
    "points, labels = transfrom(points, labels)\n",
    "show(points, labels, bb_vehicle=True)\n",
    "\n",
    "show(points_s, pred_labels, bb_vehicle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SemanticSprayDataset(\n",
    "    \"./SemanticSprayDataset/\",\n",
    "    \"test\",\n",
    "    return_xyz=False,\n",
    "    sample_size=None,\n",
    "    transfroms=CenterCrop(30)\n",
    ")\n",
    "\n",
    "points, labels = test_dataset[1399]\n",
    "\n",
    "points_s, pred_labels = predict_one(model, points, labels, transfrom=CenterCrop(30))\n",
    "    \n",
    "show(points, labels, bb_vehicle=True)\n",
    "show(points_s, pred_labels, bb_vehicle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Оригинальное облако|PointNet++ (предсказание)|\n",
    "|-|-|\n",
    "|Тестовый сэмпл|Тестовый сэмпл (предсказание)|\n",
    "|![alt](images/pointnet_orig.png) | ![alt](images/pointnet_pred.png)|\n",
    "|Сэмпл из тестового датсета|Сэмпл из тестового датсета (предсказание)|\n",
    "|![alt](images/pointnet_orig_ds.png) | ![alt](images/pointnet_pred_ds.png)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.35 s ± 74.8 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "1.62 s ± 18.4 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 10 predict_one(model, points, labels, transfrom=CenterCrop(20))\n",
    "%timeit -n 1 -r 10 predict_one(model, points, labels, transfrom=CenterCrop(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среднее время на одно предсказание для модели PointNet++ (RTX 2070 SUPER):\n",
    "1) для облака точек в области $|x| < 20$ и $|y| < 20$ ~ 1.3s\n",
    "1) для облака точек в области $|x| < 30$ и $|y| < 30$ ~ 1.6s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод по PointNet++\n",
    "\n",
    "Плюсы: \n",
    "- неплохая точность алгоритма\n",
    "\n",
    "Минусы:\n",
    "- очень долгий инференс > 1s\n",
    "\n",
    "Метрики качества:\n",
    "\n",
    "|Класс|Вес класса|IoU|\n",
    "|:-|:-|:-|\n",
    "|background|0.971|0.998|\n",
    "|foreground|0.011|0.965|\n",
    "|noise|0.018|0.925|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Третий метод (модель DGCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgcnn.train import train\n",
    "\n",
    "\n",
    "class Args:\n",
    "    lr = 1e-3\n",
    "    epochs = 17\n",
    "    from_checkpoint = False\n",
    "    scheduler = 'step'\n",
    "    step_size = 5\n",
    "    emb_dims = 1024\n",
    "    k = 20\n",
    "    in_channels = 5\n",
    "    num_points = 2048\n",
    "    batch_size = 16\n",
    "    test_batch_size = 16\n",
    "    dropout = 0.5\n",
    "    num_classes = 3\n",
    "    maxs = maxs\n",
    "    mins = mins\n",
    "    labelweights = labelweights\n",
    "\n",
    "transfroms = Compose([   \n",
    "    # RandomRotateZ(),\n",
    "    CenterCrop(30),\n",
    "    MinMaxScaler(mins=Args.mins, maxs=Args.maxs) \n",
    "])\n",
    "    \n",
    "train_dataset = SemanticSprayDataset(\n",
    "    \"./SemanticSprayDataset/\",\n",
    "    \"train\",\n",
    "    return_xyz=False,\n",
    "    sample_size=Args.num_points,\n",
    "    transfroms=transfroms\n",
    ")\n",
    "\n",
    "test_dataset = SemanticSprayDataset(\n",
    "    \"./SemanticSprayDataset/\",\n",
    "    \"test\",\n",
    "    return_xyz=False,\n",
    "    sample_size=Args.num_points,\n",
    "    transfroms=transfroms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=Args.batch_size, shuffle=True, \n",
    "                          drop_last=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Args.test_batch_size, shuffle=False, \n",
    "                         drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:53<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0, loss: 0.047669, train acc: 0.990073, train avg acc: 0.966504, train iou: 0.824045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:05<00:00,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0, loss: 0.017520, test acc: 0.997761, test avg acc: 0.975307, test iou: 0.950558\n",
      "------- IoU --------\n",
      "class background     weight: 0.971, IoU: 0.998 \n",
      "class foreground     weight: 0.011, IoU: 0.952 \n",
      "class noise          weight: 0.018, IoU: 0.902 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:48<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1, loss: 0.013555, train acc: 0.997484, train avg acc: 0.990006, train iou: 0.950803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:05<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1, loss: 0.011182, test acc: 0.997947, test avg acc: 0.990709, test iou: 0.955563\n",
      "------- IoU --------\n",
      "class background     weight: 0.971, IoU: 0.998 \n",
      "class foreground     weight: 0.011, IoU: 0.956 \n",
      "class noise          weight: 0.018, IoU: 0.913 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:52<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 2, loss: 0.011751, train acc: 0.997761, train avg acc: 0.991807, train iou: 0.956867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:05<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2, loss: 0.012278, test acc: 0.997996, test avg acc: 0.988795, test iou: 0.957065\n",
      "------- IoU --------\n",
      "class background     weight: 0.971, IoU: 0.998 \n",
      "class foreground     weight: 0.011, IoU: 0.958 \n",
      "class noise          weight: 0.018, IoU: 0.915 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:52<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 3, loss: 0.012152, train acc: 0.997687, train avg acc: 0.991813, train iou: 0.955606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:05<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3, loss: 0.011418, test acc: 0.997844, test avg acc: 0.989659, test iou: 0.953299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:52<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 4, loss: 0.011500, train acc: 0.997772, train avg acc: 0.992040, train iou: 0.957306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:06<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4, loss: 0.015328, test acc: 0.997384, test avg acc: 0.983604, test iou: 0.940606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:53<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 5, loss: 0.010105, train acc: 0.997983, train avg acc: 0.993250, train iou: 0.961999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:06<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 5, loss: 0.009766, test acc: 0.998287, test avg acc: 0.991289, test iou: 0.963958\n",
      "------- IoU --------\n",
      "class background     weight: 0.971, IoU: 0.998 \n",
      "class foreground     weight: 0.011, IoU: 0.970 \n",
      "class noise          weight: 0.018, IoU: 0.923 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:52<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 6, loss: 0.009456, train acc: 0.998167, train avg acc: 0.993692, train iou: 0.965396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:08<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 6, loss: 0.010656, test acc: 0.998006, test avg acc: 0.991428, test iou: 0.958145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:53<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 7, loss: 0.009853, train acc: 0.998072, train avg acc: 0.993092, train iou: 0.963318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:07<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 7, loss: 0.009962, test acc: 0.998247, test avg acc: 0.991572, test iou: 0.962505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:52<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 8, loss: 0.009822, train acc: 0.998109, train avg acc: 0.992888, train iou: 0.963415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:04<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 8, loss: 0.009796, test acc: 0.998133, test avg acc: 0.991338, test iou: 0.960356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:49<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 9, loss: 0.009167, train acc: 0.998172, train avg acc: 0.993727, train iou: 0.965659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:04<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 9, loss: 0.011129, test acc: 0.998235, test avg acc: 0.988585, test iou: 0.962351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:48<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 10, loss: 0.008565, train acc: 0.998279, train avg acc: 0.994089, train iou: 0.967750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:04<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 10, loss: 0.008934, test acc: 0.998290, test avg acc: 0.993085, test iou: 0.963889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:49<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 11, loss: 0.008098, train acc: 0.998358, train avg acc: 0.994329, train iou: 0.968975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:04<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 11, loss: 0.008955, test acc: 0.998365, test avg acc: 0.991848, test iou: 0.966044\n",
      "------- IoU --------\n",
      "class background     weight: 0.971, IoU: 0.998 \n",
      "class foreground     weight: 0.011, IoU: 0.973 \n",
      "class noise          weight: 0.018, IoU: 0.927 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:48<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 12, loss: 0.007972, train acc: 0.998411, train avg acc: 0.994248, train iou: 0.969713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:04<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 12, loss: 0.009232, test acc: 0.998397, test avg acc: 0.991301, test iou: 0.964971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:48<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 13, loss: 0.008287, train acc: 0.998346, train avg acc: 0.994061, train iou: 0.968676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:04<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 13, loss: 0.008867, test acc: 0.998293, test avg acc: 0.992854, test iou: 0.963552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:48<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 14, loss: 0.007951, train acc: 0.998399, train avg acc: 0.994292, train iou: 0.969636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:04<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 14, loss: 0.009415, test acc: 0.998504, test avg acc: 0.990361, test iou: 0.968031\n",
      "------- IoU --------\n",
      "class background     weight: 0.971, IoU: 0.999 \n",
      "class foreground     weight: 0.011, IoU: 0.972 \n",
      "class noise          weight: 0.018, IoU: 0.933 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:48<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 15, loss: 0.007169, train acc: 0.998525, train avg acc: 0.994718, train iou: 0.971907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:04<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 15, loss: 0.009327, test acc: 0.998472, test avg acc: 0.990673, test iou: 0.967194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:52<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 16, loss: 0.007121, train acc: 0.998572, train avg acc: 0.994793, train iou: 0.972885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [01:05<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 16, loss: 0.009272, test acc: 0.998451, test avg acc: 0.991340, test iou: 0.967424\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, test_loader, Args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980480\n"
     ]
    }
   ],
   "source": [
    "from dgcnn.model import DGCNN_semseg\n",
    "\n",
    "\n",
    "model = DGCNN_semseg(Args).cuda()\n",
    "model.load_state_dict(torch.load(\"./models/dgcnn_model_best.t7\"))\n",
    "model.eval()\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_one(model, points, labels, args=Args, transfrom=CenterCrop(30)):\n",
    "    \"\"\"Предсказание производится батчами размером по args.NUM_POINT для DGCNN. \n",
    "    Данный размер облака точек использовался во время обучения. \n",
    "    \"\"\"\n",
    "    points = points[:, :args.in_channels]\n",
    "    if transfrom:\n",
    "        points, labels = transfrom(points, labels)\n",
    "\n",
    "    idxs = np.random.permutation(np.arange(1, len(labels)))\n",
    "    points, labels = points[idxs, :], labels[idxs]\n",
    "\n",
    "    n_points = points.shape[0]\n",
    "    prop_size = args.num_points - (n_points % args.num_points)\n",
    "    points_prop = np.zeros((n_points + prop_size, points.shape[1]))\n",
    "    points_prop[:n_points, :] = points\n",
    "    points_prop[n_points:, :] = points[np.random.choice(labels.size, prop_size, replace=False), :]\n",
    "    \n",
    "    preds_all = []\n",
    "    for i in range(0, n_points, args.num_points):\n",
    "\n",
    "        points_sample = points[i:i+args.num_points, :]\n",
    "        points_sample = MinMaxScaler(mins=args.mins, maxs=args.maxs)(points_sample, labels)[0][None, :, :]\n",
    "\n",
    "        points_sample = torch.Tensor(points_sample)\n",
    "        points_sample = points_sample.float().cuda()\n",
    "        points_sample = points_sample.permute(0, 2, 1)\n",
    "\n",
    "        seg_pred = model(points_sample)\n",
    "        seg_pred = seg_pred.permute(0, 2, 1).contiguous()\n",
    "        pred_val = seg_pred.view(-1, Args.num_classes).detach().cpu().numpy()\n",
    "        pred_val = np.argmax(pred_val, 1)\n",
    "\n",
    "        preds_all.append(pred_val)\n",
    "\n",
    "    return points, np.hstack(preds_all)[:n_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_id = \"000050\"\n",
    "points = np.fromfile(f\"{cloud_id}.bin\", np.float32).reshape(-1, 5)\n",
    "labels = np.fromfile(f\"{cloud_id}.label\", np.int32).reshape(-1)\n",
    "\n",
    "points_s, pred_labels = predict_one(model, points, labels, transfrom=CenterCrop(30))\n",
    "    \n",
    "transfrom = CenterCrop(30)\n",
    "points, labels = transfrom(points, labels)\n",
    "show(points, labels, bb_vehicle=True)\n",
    "\n",
    "show(points_s, pred_labels, bb_vehicle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SemanticSprayDataset(\n",
    "    \"./SemanticSprayDataset/\",\n",
    "    \"test\",\n",
    "    return_xyz=False,\n",
    "    sample_size=None,\n",
    "    transfroms=CenterCrop(30)\n",
    ")\n",
    "\n",
    "points, labels = test_dataset[1399]\n",
    "\n",
    "points_s, pred_labels = predict_one(model, points, labels, transfrom=CenterCrop(30))\n",
    "    \n",
    "show(points, labels, bb_vehicle=True)\n",
    "show(points_s, pred_labels, bb_vehicle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Оригинальное облако|DGCNN (предсказание)|\n",
    "|-|-|\n",
    "|Тестовый сэмпл|Тестовый сэмпл (предсказание)|\n",
    "|![alt](images/dgcnn_orig.png) | ![alt](images/dgcnn_pred.png)|\n",
    "|Сэмпл из тестового датсета|Сэмпл из тестового датсета (предсказание)|\n",
    "|![alt](images/dgcnn_orig_ds.png) | ![alt](images/dgcnn_pred_ds.png)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.5 ms ± 1.19 ms per loop (mean ± std. dev. of 100 runs, 5 loops each)\n",
      "68.9 ms ± 1.46 ms per loop (mean ± std. dev. of 100 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 5 -r 100 predict_one(model, points, labels, transfrom=CenterCrop(20))\n",
    "%timeit -n 5 -r 100 predict_one(model, points, labels, transfrom=CenterCrop(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среднее время на одно предсказание для модели DGCNN (RTX 2070 SUPER):\n",
    "1) для облака точек в области $|x| < 20$ и $|y| < 20$ ~ 49ms\n",
    "2) для облака точек в области $|x| < 30$ и $|y| < 30$ ~ 68ms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод по DGCNN\n",
    "\n",
    "Плюсы: \n",
    "1) неплохая точность алгоритма \n",
    "2) приемлемый инференс `~49ms` на ограниченном облаке точек\n",
    "\n",
    "Минусы:\n",
    "1) всё ещё медленный инференс для полного облака точек\n",
    "\n",
    "Метрики качества:\n",
    "\n",
    "|Класс|Вес класса|IoU|\n",
    "|:-|:-|:-|\n",
    "|background|0.971|0.999|\n",
    "|foreground|0.011|0.972|\n",
    "|noise|0.018|0.933|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод:\n",
    "\n",
    "В качестве финального алгоритма предлагается использование модели DGCNN, которая в рамках данной задачи удовлетворяет поставленным условиям. \n",
    "Модель показывает высокое качество сегментации для всех трёх классов (по метрике IoU и визуально), а также инференс модели на ограниченном облаке точек менее 50ms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  4 19:52:25 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 537.58                 Driver Version: 537.58       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2070 ...  WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 53%   32C    P8               8W / 215W |   6744MiB /  8192MiB |     18%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4400    C+G   ...aming\\Telegram Desktop\\Telegram.exe    N/A      |\n",
      "|    0   N/A  N/A      5744    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6484    C+G   ...conda3\\envs\\torch_env_23\\python.exe    N/A      |\n",
      "|    0   N/A  N/A      8588    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8744    C+G   ...ndexBrowser\\Application\\browser.exe    N/A      |\n",
      "|    0   N/A  N/A      8772    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      9292    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10168    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11108    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     11480    C+G   ...on\\120.0.2210.91\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     14960    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     15424    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15944    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env_23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
